{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "144c6fc5",
   "metadata": {
    "tags": [
     "header"
    ]
   },
   "source": [
    "# STA 220 Assignment 2\n",
    "\n",
    "Due __Februrary 9, 2024__ by __11:59pm__. Submit your work by uploading it to Gradescope through Canvas.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "1. Provide your solutions in new cells following each exercise description. Create as many new cells as necessary. Use code cells for your Python scripts and Markdown cells for explanatory text or answers to non-coding questions. Answer all textual questions in complete sentences.\n",
    "2. The use of assistive tools is permitted, but must be indicated. You will be graded on you proficiency in coding. Produce high quality code by adhering to proper programming principles. \n",
    "3. Export the .jpynb as .pdf and submit it on Gradescope in time. To facilitate grading, indicate the area of the solution on the submission. Submissions without indication will be marked down. No late submissions accepted. \n",
    "4. If test cases are given, your solution must be in the same format. \n",
    "5. The total number of points is 10. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe607f59",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "__Exercise 1__\n",
    "\n",
    "We will compute the [PageRank](https://en.wikipedia.org/wiki/PageRank) of the articles of the [Sinhala](https://en.wikipedia.org/wiki/Sinhala_language) wikipedia, which is available at [si.wikipedia.org](https://si.wikipedia.org/wiki/%E0%B6%B8%E0%B7%94%E0%B6%BD%E0%B7%8A_%E0%B6%B4%E0%B7%92%E0%B6%A7%E0%B7%94%E0%B7%80). Additional information of the Sinhala wiki can be found [here](https://meta.wikimedia.org/wiki/List_of_Wikipedias). \n",
    "\n",
    "_Hints: If you don't speak Sinhalese, you might want to learn the wiki logic from the english wikipedia, and translate your findings. Also, caching is highly recommended._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f276cd17",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "__(a)__ Use the special [AllPages](https://si.wikipedia.org/wiki/%E0%B7%80%E0%B7%92%E0%B7%81%E0%B7%9A%E0%B7%82:%E0%B7%83%E0%B7%92%E0%B6%BA%E0%B7%85%E0%B7%94_%E0%B6%B4%E0%B7%92%E0%B6%A7%E0%B7%94) page and understand its logic to retrieve the url of all articles in the sinhalese wikipedia. Make sure to skip redirections.\n",
    "\n",
    "_How many articles are there?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c3844f2-e1ed-4c01-bd72-8d0f9fd3acbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#(a)\n",
    "import requests\n",
    "import lxml.html as lx\n",
    "import requests_cache\n",
    "requests_cache.install_cache('wiki_cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75e55b1d-4208-404a-8887-ec4b531e8044",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "article_urls= []\n",
    "base_url = \"https://si.wikipedia.org\"\n",
    "\n",
    "def get_all_article_urls(base_url):\n",
    "    \n",
    "    # first page\n",
    "    current_url='https://si.wikipedia.org/w/index.php?title=විශේෂ:සියළු_පිටු&from=%22අන්තර්+ජාලය+තුල%2C+ඔබ+බල්ලෙකු+බව+කිසිවෙක්+නොදනි'\n",
    "    response = requests.get(current_url)\n",
    "    tree = lx.fromstring(response.content)\n",
    "    articles = tree.xpath('//*[@id=\"mw-content-text\"]/div[3]//a[not(contains(@class, \"mw-redirect\"))]/@href') #skip redirect\n",
    "    for article in articles:\n",
    "        urls = [base_url + article]\n",
    "        article_urls.append(urls)\n",
    "    \n",
    "\n",
    "    # second to last \n",
    "    current_url = base_url + \"/w/index.php?title=විශේෂ:සියළු_පිටු&from=1880+ගණන්\"\n",
    "  \n",
    "    while current_url:\n",
    "        response = requests.get(current_url)\n",
    "        tree = lx.fromstring(response.content)\n",
    "        \n",
    "        # Extract article URLs from the current page\n",
    "        articles = tree.xpath('//*[@id=\"mw-content-text\"]/div[3]//a[not(contains(@class, \"mw-redirect\"))]/@href') #skip redirect\n",
    "        for article in articles:\n",
    "            urls = [base_url + article]\n",
    "            article_urls.append(urls)\n",
    "        \n",
    "        # Check for the next page link\n",
    "        next_page=tree.xpath('//*[@id=\"mw-content-text\"]/div[2]/a/@href')\n",
    "        try:\n",
    "            if next_page:\n",
    "                current_url = base_url + next_page[1]\n",
    "            else:\n",
    "                current_url = None\n",
    "        except IndexError:\n",
    "            print(\"Reached last page\")\n",
    "            current_url = None\n",
    "\n",
    "    return article_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ac0b221-ce6c-4efe-841c-029cce0f8948",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached last page\n"
     ]
    }
   ],
   "source": [
    "# a \n",
    "get_all_article_urls(base_url)\n",
    "flat_total_article= [item for sublist in article_urls for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f6c4cd49-4c5c-4aaa-b390-e1a5f75c9091",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are total 24233 pages without redirections\n"
     ]
    }
   ],
   "source": [
    "#a\n",
    "print(f'There are total {len(flat_total_article)} pages without redirections')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90213dd",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "__(b, i)__ Scan all articles in the sinhalese wikipedia and retrieve all links to other articles. Avoid links to special pages, images or the ones that point to another website. Only count the proper article for links that point to a specific section. Use regular expressions to manage these cases. \n",
    "__(ii)__ Make sure to match redirections to their correct destiation article. To this end, find how wikipedia treats redirections and retrieve the true article. _(Help: Try searching for 'uc davis' on en.wikipedia.org')_\n",
    "__(iii)__ Use threading to request all articles and obtain all links to other articles. _(Attention: This takes about thirty minutes!)_\n",
    "\n",
    "\n",
    "_How many links to other articles are there?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "36366f5f-2648-42a3-ad87-a6f1e2b04189",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#(b,i) solution\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def get_all_article_links(urls):\n",
    "    article_links = {}\n",
    "    for url in urls:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        content_div = soup.find(id='mw-content-text')\n",
    "        if content_div:\n",
    "            links = content_div.find_all('a', href=True)\n",
    "            link_list = []\n",
    "            for link in links:\n",
    "                href = link['href']\n",
    "                # Use regular expressions to filter out unwanted links\n",
    "                if not re.match(r'^/wiki/(?!Special:|%[A-F0-9]{2}).*$', href) \\\n",
    "                        and not re.match(r'^https?://.+\\.wikipedia\\.org/wiki/.+$', href) \\\n",
    "                        and not re.search(r'\\.(jpg|jpeg|png|gif|svg)$', href) \\\n",
    "                        and not re.search(r':Cite', href) \\\n",
    "                        and not re.match(r'^/wiki/File:.*$', href) \\\n",
    "                        and not re.search(r'action=edit|redlink=1', href) \\\n",
    "                        and re.match(r'^/wiki/.+', href) \\\n",
    "                        and not '#' in href:\n",
    "                    link_list.append(base_url+href)\n",
    "            article_links[url] = link_list\n",
    "    return article_links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "604fc33d-94ad-4711-a989-2b95ea42b5b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#(b,ii) solution\n",
    "\n",
    "def filter_existing_links(existing_links, article_links):\n",
    "    updated_links = {}\n",
    "\n",
    "    for url, links in existing_links.items():\n",
    "        # Filter links that are present in article_links\n",
    "        filtered_links = [link for link in links if link in article_links]\n",
    "        # Check if any filtered links are present\n",
    "        if filtered_links:\n",
    "            updated_links[url] = filtered_links\n",
    "    return updated_links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cdf0891c-8249-4fd5-a362-7a03796795c1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#(b,iii)\n",
    "\n",
    "article_links=get_all_article_links(flat_total_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "50c2dfe2-f4a8-4a46-807b-3274dc4f3aae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#(b,iii) \n",
    "updated_links=filter_existing_links(article_links,flat_total_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0aaa507b-1707-4f1e-bc06-1f1554c7b6bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of links to other articles: 303237\n"
     ]
    }
   ],
   "source": [
    "#(b,iii) solution ,Kernel dies when using threading \n",
    "total_values = sum(len(value) for value in updated_links.values())\n",
    "print(\"Total number of links to other articles:\", total_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59293f1b-90fa-4b15-845e-2c8c0711710d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#(b,iii) code with threading \n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "def process_links(urls):\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        results = executor.map(get_all_article_links, urls)\n",
    "\n",
    "article_links_dict = process_links()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1775e365",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "__(c)__ Compute the transition matrix (see [here](https://en.wikipedia.org/wiki/Google_matrix) and [here](https://www.amsi.org.au/teacher_modules/pdfs/Maths_delivers/Pagerank5.pdf) for step-by-step instructions). Make sure to tread dangling nodes. You may want to use: \n",
    "```\n",
    "from scipy.sparse import csr_matrix\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9333687b-1182-4397-babd-48ba1facfde0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#(c) solution\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def compute_transition_matrix(links):\n",
    "    all_pages = set(links.keys())\n",
    "    linked_pages = set(page for linked_pages in links.values() for page in linked_pages)\n",
    "    dangling_nodes = all_pages - linked_pages\n",
    "    num_pages = len(all_pages)\n",
    "    transition_matrix = np.zeros((num_pages, num_pages))\n",
    "       \n",
    "    for i, page in enumerate(all_pages):\n",
    "        num_outlinks = len(links[page])\n",
    "        if num_outlinks == 0:  \n",
    "            transition_matrix[i] = 1 / num_pages\n",
    "        else:\n",
    "            for linked_page in links[page]:\n",
    "                if linked_page in all_pages:  \n",
    "                    j = list(all_pages).index(linked_page)\n",
    "                    transition_matrix[i][j] = 1 / num_outlinks\n",
    "    \n",
    "\n",
    "    return csr_matrix(transition_matrix)\n",
    "\n",
    "\n",
    "transition_matrix = compute_transition_matrix(updated_links)\n",
    "print(transition_matrix.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be68fe99",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "__(d, i)__ Set the damping factor to `0.85` and compute the PageRank for each article, using fourty iterations and starting with a vector with equal entries. __(ii)__ Obtain the top ten articles in terms of PageRank, and, retrieving the articles again, find the correponding English article, if available. \n",
    "\n",
    "_Return the corresponding English article titles of the top ten articles from the Sinhalese wikipedia._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "23c5f7fb-2d2e-4855-9e21-e35da6872431",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.73616882e-05 3.67762696e-05 2.55561583e-05 ... 8.75401226e-06\n",
      " 3.53263591e-05 2.47862561e-05]\n"
     ]
    }
   ],
   "source": [
    "# (d,i) solution\n",
    "\n",
    "def compute_pagerank(transition_matrix, damping_factor=0.85, num_iterations=40):\n",
    "    num_pages = transition_matrix.shape[0]\n",
    "    pagerank = np.ones(num_pages) / num_pages\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        pagerank = (1 - damping_factor) / num_pages + damping_factor * transition_matrix.dot(pagerank)\n",
    "\n",
    "    return pagerank\n",
    "\n",
    "pagerank = compute_pagerank(transition_matrix, damping_factor=0.85, num_iterations=40)\n",
    "print(pagerank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "166c763c-3a89-42c8-9141-4cdde26c45be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://si.wikipedia.org/wiki/Sum\n",
      "https://si.wikipedia.org/wiki/Scutigera_coleoptrata\n",
      "https://si.wikipedia.org/wiki/Pseudophilautus_asankai\n",
      "https://si.wikipedia.org/wiki/992\n",
      "https://si.wikipedia.org/wiki/5_%E0%B7%80%E0%B6%B1_%E0%B6%86%E0%B7%80%E0%B6%BB%E0%B7%8A%E0%B6%AD%E0%B6%BA%E0%B7%9A_%E0%B6%B8%E0%B7%96%E0%B6%BD%E0%B6%AF%E0%B7%8A%E2%80%8D%E0%B6%BB%E0%B7%80%E0%B7%8A%E2%80%8D%E0%B6%BA%E0%B6%BA\n",
      "https://si.wikipedia.org/wiki/2021_%E0%B6%BA%E0%B7%94%E0%B6%BB%E0%B7%9D%E0%B6%B4%E0%B7%92%E0%B6%BA%E0%B7%8F%E0%B6%B1%E0%B7%94_%E0%B6%A2%E0%B6%BD%E0%B6%9C%E0%B7%90%E0%B6%BD%E0%B7%94%E0%B6%B8%E0%B7%8A\n",
      "https://si.wikipedia.org/wiki/1812_%E0%B6%BA%E0%B7%94%E0%B6%AF%E0%B7%8A%E0%B6%B0%E0%B6%BA\n",
      "https://si.wikipedia.org/wiki/150_%E0%B6%B8%E0%B7%94%E0%B6%BD%E0%B7%8A%E0%B6%BD%E0%B7%9A%E0%B6%BB%E0%B7%92%E0%B6%BA%E0%B7%8F%E0%B7%80\n",
      "https://si.wikipedia.org/wiki/13\n",
      "https://si.wikipedia.org/wiki/%E0%B7%86%E0%B7%92%E0%B6%B6%E0%B7%9C%E0%B6%B1%E0%B7%8F%E0%B6%A0%E0%B7%8A%E0%B6%A0%E0%B7%92_%E0%B7%83%E0%B6%82%E0%B6%9B%E0%B7%8A%E2%80%8D%E0%B6%BA%E0%B7%8F_%E0%B6%B7%E0%B7%8F%E0%B7%80%E0%B7%92%E0%B6%AD%E0%B6%BA%E0%B6%B1%E0%B7%8A\n"
     ]
    }
   ],
   "source": [
    "# (d,ii) solution\n",
    "\n",
    "def top_10_page_urls(pagerank_scores, urls):\n",
    "    pagerank_urls = zip(pagerank_scores, urls)\n",
    "    sorted_pagerank_urls = sorted(pagerank_urls, reverse=True)\n",
    "    top_10_urls = [url for _, url in sorted_pagerank_urls[:10]]\n",
    "    return top_10_urls\n",
    "\n",
    "pagerank_scores = compute_pagerank(transition_matrix)\n",
    "top_10_urls = top_10_page_urls(pagerank_scores, list(updated_links.keys()))\n",
    "\n",
    "for url in top_10_urls:\n",
    "    print(url)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
